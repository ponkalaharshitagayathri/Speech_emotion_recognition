{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# Import libraries \n","import librosa\n","import librosa.display\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from matplotlib.pyplot import specgram\n","import pandas as pd\n","import glob \n","from sklearn.metrics import confusion_matrix\n","import IPython.display as ipd  # To play sound in the notebook\n","import os\n","import sys\n","import warnings\n","# ignore warnings \n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#for dirname, _, filenames in os.walk('/kaggle/input'):\n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","\n","TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n","RAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n","SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n","CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n","\n","# Run one example \n","dir_list = os.listdir(SAVEE)\n","dir_list[0:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get the data location for SAVEE\n","dir_list = os.listdir(SAVEE)\n","\n","# parse the filename to get the emotions\n","emotion=[]\n","path = []\n","for i in dir_list:\n","    if i[-8:-6]=='_a':\n","        emotion.append('male_angry')\n","    elif i[-8:-6]=='_d':\n","        emotion.append('male_disgust')\n","    elif i[-8:-6]=='_f':\n","        emotion.append('male_fear')\n","    elif i[-8:-6]=='_h':\n","        emotion.append('male_happy')\n","    elif i[-8:-6]=='_n':\n","        emotion.append('male_neutral')\n","    elif i[-8:-6]=='sa':\n","        emotion.append('male_sad')\n","    elif i[-8:-6]=='su':\n","        emotion.append('male_surprise')\n","    else:\n","        emotion.append('male_error') \n","    path.append(SAVEE + i)\n","\n","# Now check out the label count distribution \n","SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n","SAVEE_df['source'] = 'SAVEE'\n","SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n","SAVEE_df.labels.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# use the well known Librosa library for this task \n","fname = SAVEE + 'DC_f11.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Lets play a happy track\n","fname = SAVEE + 'DC_h11.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dir_list = os.listdir(RAV)\n","dir_list.sort()\n","\n","emotion = []\n","gender = []\n","path = []\n","for i in dir_list:\n","    fname = os.listdir(RAV + i)\n","    for f in fname:\n","        part = f.split('.')[0].split('-')\n","        emotion.append(int(part[2]))\n","        temp = int(part[6])\n","        if temp%2 == 0:\n","            temp = \"female\"\n","        else:\n","            temp = \"male\"\n","        gender.append(temp)\n","        path.append(RAV + i + '/' + f)\n","\n","        \n","RAV_df = pd.DataFrame(emotion)\n","RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n","RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n","RAV_df.columns = ['gender','emotion']\n","RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n","RAV_df['source'] = 'RAVDESS'  \n","RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","RAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\n","RAV_df.labels.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Pick a fearful track\n","fname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Pick a happy track\n","fname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dir_list = os.listdir(TESS)\n","dir_list.sort()\n","dir_list"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path = []\n","emotion = []\n","\n","for i in dir_list:\n","    fname = os.listdir(TESS + i)\n","    for f in fname:\n","        if i == 'OAF_angry' or i == 'YAF_angry':\n","            emotion.append('female_angry')\n","        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n","            emotion.append('female_disgust')\n","        elif i == 'OAF_Fear' or i == 'YAF_fear':\n","            emotion.append('female_fear')\n","        elif i == 'OAF_happy' or i == 'YAF_happy':\n","            emotion.append('female_happy')\n","        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n","            emotion.append('female_neutral')                                \n","        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n","            emotion.append('female_surprise')               \n","        elif i == 'OAF_Sad' or i == 'YAF_sad':\n","            emotion.append('female_sad')\n","        else:\n","            emotion.append('Unknown')\n","        path.append(TESS + i + \"/\" + f)\n","\n","TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n","TESS_df['source'] = 'TESS'\n","TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","TESS_df.labels.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lets play a fearful track \n","fname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n","\n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lets play a happy track \n","fname =  TESS + 'YAF_happy/YAF_dog_happy.wav' \n","\n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dir_list = os.listdir(CREMA)\n","dir_list.sort()\n","print(dir_list[0:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gender = []\n","emotion = []\n","path = []\n","female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n","          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n","\n","for i in dir_list: \n","    part = i.split('_')\n","    if int(part[0]) in female:\n","        temp = 'female'\n","    else:\n","        temp = 'male'\n","    gender.append(temp)\n","    if part[2] == 'SAD' and temp == 'male':\n","        emotion.append('male_sad')\n","    elif part[2] == 'ANG' and temp == 'male':\n","        emotion.append('male_angry')\n","    elif part[2] == 'DIS' and temp == 'male':\n","        emotion.append('male_disgust')\n","    elif part[2] == 'FEA' and temp == 'male':\n","        emotion.append('male_fear')\n","    elif part[2] == 'HAP' and temp == 'male':\n","        emotion.append('male_happy')\n","    elif part[2] == 'NEU' and temp == 'male':\n","        emotion.append('male_neutral')\n","    elif part[2] == 'SAD' and temp == 'female':\n","        emotion.append('female_sad')\n","    elif part[2] == 'ANG' and temp == 'female':\n","        emotion.append('female_angry')\n","    elif part[2] == 'DIS' and temp == 'female':\n","        emotion.append('female_disgust')\n","    elif part[2] == 'FEA' and temp == 'female':\n","        emotion.append('female_fear')\n","    elif part[2] == 'HAP' and temp == 'female':\n","        emotion.append('female_happy')\n","    elif part[2] == 'NEU' and temp == 'female':\n","        emotion.append('female_neutral')\n","    else:\n","        emotion.append('Unknown')\n","    path.append(CREMA + i)\n","    \n","CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n","CREMA_df['source'] = 'CREMA'\n","CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","CREMA_df.labels.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# use the well known Librosa library for this task \n","fname = CREMA + '1012_IEO_HAP_HI.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# A fearful track\n","fname = CREMA + '1012_IEO_FEA_HI.wav'  \n","data, sampling_rate = librosa.load(fname)\n","plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)\n","\n","# Lets play the audio \n","ipd.Audio(fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\n","print(df.labels.value_counts())\n","df.head()\n","df.to_csv(\"Data_path.csv\",index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Import our libraries\n","import librosa\n","import librosa.display\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import specgram\n","import pandas as pd\n","import os\n","import IPython.display as ipd  # To play sound in the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Female; Emotion - Angry \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","\n","# audio wave\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.waveplot(X, sr=sample_rate)\n","plt.title('Audio sampled at 44100 hrz')\n","\n","# MFCC\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.specshow(mfcc, x_axis='time')\n","plt.ylabel('MFCC')\n","plt.colorbar()\n","\n","ipd.Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Male; Emotion - Angry \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","\n","# audio wave\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.waveplot(X, sr=sample_rate)\n","plt.title('Audio sampled at 44100 hrz')\n","\n","# MFCC\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.specshow(mfcc, x_axis='time')\n","plt.ylabel('MFCC')\n","plt.colorbar()\n","\n","ipd.Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Female; Emotion - Happy \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","\n","# audio wave\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.waveplot(X, sr=sample_rate)\n","plt.title('Audio sampled at 44100 hrz')\n","\n","# MFCC\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.specshow(mfcc, x_axis='time')\n","plt.ylabel('MFCC')\n","plt.colorbar()\n","\n","ipd.Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Male; Emotion - Happy \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","\n","# audio wave\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.waveplot(X, sr=sample_rate)\n","plt.title('Audio sampled at 44100 hrz')\n","\n","# MFCC\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","librosa.display.specshow(mfcc, x_axis='time')\n","plt.ylabel('MFCC')\n","plt.colorbar()\n","\n","ipd.Audio(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Female; Emotion - Angry \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n","print(len(female))\n","\n","# Source - RAVDESS; Gender - Male; Emotion - Angry \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n","print(len(male))\n","\n","# audio wave\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","plt.plot(female, label='female')\n","plt.plot(male, label='male')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Source - RAVDESS; Gender - Female; Emotion - happy \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n","print(len(female))\n","\n","# Source - RAVDESS; Gender - Male; Emotion - happy \n","path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\n","X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n","male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n","male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n","print(len(male))\n","\n","# Plot the two audio waves together\n","plt.figure(figsize=(20, 15))\n","plt.subplot(3,1,1)\n","plt.plot(female, label='female')\n","plt.plot(male, label='male')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Importing required libraries \n","# Keras\n","import keras\n","from keras import regularizers\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential, Model, model_from_json\n","from keras.layers import Dense, Embedding, LSTM\n","from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n","from keras.utils import np_utils, to_categorical\n","from keras.callbacks import ModelCheckpoint\n","\n","# sklearn\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Other  \n","import librosa\n","import librosa.display\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from matplotlib.pyplot import specgram\n","import pandas as pd\n","import seaborn as sns\n","import glob \n","import os\n","import pickle\n","import IPython.display as ipd  # To play sound in the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# lets pick up the meta-data that we got from our first part of the Kernel\n","ref = pd.read_csv(\"/kaggle/input/data-pathcsv/Data_path.csv\")\n","ref.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"outputs":[],"source":["# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \n","df = pd.DataFrame(columns=['feature'])\n","\n","# loop feature extraction over the entire dataset\n","counter=0\n","for index,path in enumerate(ref.path):\n","    X, sample_rate = librosa.load(path\n","                                  , res_type='kaiser_fast'\n","                                  ,duration=2.5\n","                                  ,sr=44100\n","                                  ,offset=0.5\n","                                 )\n","    sample_rate = np.array(sample_rate)\n","    \n","    # mean as the feature. Could do min and max etc as well. \n","    mfccs = np.mean(librosa.feature.mfcc(y=X, \n","                                        sr=sample_rate, \n","                                        n_mfcc=13),\n","                    axis=0)\n","    df.loc[counter] = [mfccs]\n","    counter=counter+1   \n","\n","# Check a few records to make sure its processed successfully\n","print(len(df))\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Now extract the mean bands to its own feature columns\n","df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n","df[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# replace NA with 0\n","df=df.fillna(0)\n","print(df.shape)\n","df[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Split between train and test \n","X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n","                                                    , df.labels\n","                                                    , test_size=0.25\n","                                                    , shuffle=True\n","                                                    , random_state=42\n","                                                   )\n","\n","# Lets see how the data present itself before normalisation \n","X_train[150:160]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Lts do data normalization \n","mean = np.mean(X_train, axis=0)\n","std = np.std(X_train, axis=0)\n","\n","X_train = (X_train - mean)/std\n","X_test = (X_test - mean)/std\n","\n","# Check the dataset now \n","X_train[150:160]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Lets few preparation steps to get it into the correct format for Keras \n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)\n","\n","# one hot encode the target \n","lb = LabelEncoder()\n","y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n","y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n","\n","print(X_train.shape)\n","print(lb.classes_)\n","#print(y_train[0:10])\n","#print(y_test[0:10])\n","\n","# Pickel the lb object for future use \n","filename = 'labels'\n","outfile = open(filename,'wb')\n","pickle.dump(lb,outfile)\n","outfile.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_train = np.expand_dims(X_train, axis=2)\n","X_test = np.expand_dims(X_test, axis=2)\n","X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# New model\n","model = Sequential()\n","model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n","model.add(Activation('relu'))\n","model.add(Conv1D(256, 8, padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.25))\n","model.add(MaxPooling1D(pool_size=(8)))\n","model.add(Conv1D(128, 8, padding='same'))\n","model.add(Activation('relu'))\n","model.add(Conv1D(128, 8, padding='same'))\n","model.add(Activation('relu'))\n","model.add(Conv1D(128, 8, padding='same'))\n","model.add(Activation('relu'))\n","model.add(Conv1D(128, 8, padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.25))\n","model.add(MaxPooling1D(pool_size=(8)))\n","model.add(Conv1D(64, 8, padding='same'))\n","model.add(Activation('relu'))\n","model.add(Conv1D(64, 8, padding='same'))\n","model.add(Activation('relu'))\n","model.add(Flatten())\n","model.add(Dense(14)) # Target class number\n","model.add(Activation('softmax'))\n","# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n","# opt = keras.optimizers.Adam(lr=0.0001)\n","opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n","model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(model_history.history['loss'])\n","plt.plot(model_history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save model and weights\n","model_name = 'Emotion_Model.h5'\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","model_path = os.path.join(save_dir, model_name)\n","model.save(model_path)\n","print('Save model and weights at %s ' % model_path)\n","\n","# Save the model to disk\n","model_json = model.to_json()\n","with open(\"model_json.json\", \"w\") as json_file:\n","    json_file.write(model_json)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loading json and model architecture \n","json_file = open('model_json.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n","print(\"Loaded model from disk\")\n"," \n","# Keras optimiser\n","opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n","loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","score = loaded_model.evaluate(X_test, y_test, verbose=0)\n","print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds = loaded_model.predict(X_test, \n","                         batch_size=16, \n","                         verbose=1)\n","\n","preds=preds.argmax(axis=1)\n","preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# predictions \n","preds = preds.astype(int).flatten()\n","preds = (lb.inverse_transform((preds)))\n","preds = pd.DataFrame({'predictedvalues': preds})\n","\n","# Actual labels\n","actual=y_test.argmax(axis=1)\n","actual = actual.astype(int).flatten()\n","actual = (lb.inverse_transform((actual)))\n","actual = pd.DataFrame({'actualvalues': actual})\n","\n","# Lets combined both of them into a single dataframe\n","finaldf = actual.join(preds)\n","finaldf[170:180]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Write out the predictions to disk\n","finaldf.to_csv('Predictions.csv', index=False)\n","finaldf.groupby('predictedvalues').count()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# the confusion matrix heat map plot\n","def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n","    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n","    \n","    Arguments\n","    ---------\n","    confusion_matrix: numpy.ndarray\n","        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n","        Similarly constructed ndarrays can also be used.\n","    class_names: list\n","        An ordered list of class names, in the order they index the given confusion matrix.\n","    figsize: tuple\n","        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n","        the second determining the vertical size. Defaults to (10,7).\n","    fontsize: int\n","        Font size for axes labels. Defaults to 14.\n","        \n","    Returns\n","    -------\n","    matplotlib.figure.Figure\n","        The resulting confusion matrix figure\n","    \"\"\"\n","    df_cm = pd.DataFrame(\n","        confusion_matrix, index=class_names, columns=class_names, \n","    )\n","    fig = plt.figure(figsize=figsize)\n","    try:\n","        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n","    except ValueError:\n","        raise ValueError(\"Confusion matrix values must be integers.\")\n","        \n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","# Gender recode function\n","def gender(row):\n","    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n","        return 'female'\n","    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n","        return 'male'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get the predictions file \n","finaldf = pd.read_csv(\"Predictions.csv\")\n","classes = finaldf.actualvalues.unique()\n","classes.sort()    \n","\n","# Confusion matrix \n","c = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\n","print(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\n","print_confusion_matrix(c, class_names = classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Classification report \n","classes = finaldf.actualvalues.unique()\n","classes.sort()    \n","print(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["modidf = finaldf\n","modidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n","                                       , 'female_disgust':'female'\n","                                       , 'female_fear':'female'\n","                                       , 'female_happy':'female'\n","                                       , 'female_sad':'female'\n","                                       , 'female_surprise':'female'\n","                                       , 'female_neutral':'female'\n","                                       , 'male_angry':'male'\n","                                       , 'male_fear':'male'\n","                                       , 'male_happy':'male'\n","                                       , 'male_sad':'male'\n","                                       , 'male_surprise':'male'\n","                                       , 'male_neutral':'male'\n","                                       , 'male_disgust':'male'\n","                                      })\n","\n","modidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n","                                       , 'female_disgust':'female'\n","                                       , 'female_fear':'female'\n","                                       , 'female_happy':'female'\n","                                       , 'female_sad':'female'\n","                                       , 'female_surprise':'female'\n","                                       , 'female_neutral':'female'\n","                                       , 'male_angry':'male'\n","                                       , 'male_fear':'male'\n","                                       , 'male_happy':'male'\n","                                       , 'male_sad':'male'\n","                                       , 'male_surprise':'male'\n","                                       , 'male_neutral':'male'\n","                                       , 'male_disgust':'male'\n","                                      })\n","\n","classes = modidf.actualvalues.unique()  \n","classes.sort() \n","\n","# Confusion matrix \n","c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n","print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n","print_confusion_matrix(c, class_names = classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Classification report \n","classes = modidf.actualvalues.unique()\n","classes.sort()    \n","print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["modidf = pd.read_csv(\"Predictions.csv\")\n","modidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n","                                       , 'female_disgust':'disgust'\n","                                       , 'female_fear':'fear'\n","                                       , 'female_happy':'happy'\n","                                       , 'female_sad':'sad'\n","                                       , 'female_surprise':'surprise'\n","                                       , 'female_neutral':'neutral'\n","                                       , 'male_angry':'angry'\n","                                       , 'male_fear':'fear'\n","                                       , 'male_happy':'happy'\n","                                       , 'male_sad':'sad'\n","                                       , 'male_surprise':'surprise'\n","                                       , 'male_neutral':'neutral'\n","                                       , 'male_disgust':'disgust'\n","                                      })\n","\n","modidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n","                                       , 'female_disgust':'disgust'\n","                                       , 'female_fear':'fear'\n","                                       , 'female_happy':'happy'\n","                                       , 'female_sad':'sad'\n","                                       , 'female_surprise':'surprise'\n","                                       , 'female_neutral':'neutral'\n","                                       , 'male_angry':'angry'\n","                                       , 'male_fear':'fear'\n","                                       , 'male_happy':'happy'\n","                                       , 'male_sad':'sad'\n","                                       , 'male_surprise':'surprise'\n","                                       , 'male_neutral':'neutral'\n","                                       , 'male_disgust':'disgust'\n","                                      })\n","\n","classes = modidf.actualvalues.unique() \n","classes.sort() \n","\n","# Confusion matrix \n","c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n","print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n","print_confusion_matrix(c, class_names = classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Classification report \n","classes = modidf.actualvalues.unique()\n","classes.sort()    \n","print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.models import Sequential, Model, model_from_json\n","import matplotlib.pyplot as plt\n","import keras \n","import pickle\n","import wave  # !pip install wave\n","import os\n","import pandas as pd\n","import numpy as np\n","import sys\n","import warnings\n","import librosa\n","import librosa.display\n","import IPython.display as ipd  # To play sound in the notebook\n","\n","# ignore warnings \n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data, sampling_rate = librosa.load('/kaggle/input/happy-audio/Liza-happy-v3.wav')\n","ipd.Audio('/kaggle/input/happy-audio/Liza-happy-v3.wav')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 5))\n","librosa.display.waveplot(data, sr=sampling_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loading json and model architecture \n","json_file = open('/kaggle/input/saved-model/model_json.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","loaded_model.load_weights(\"/kaggle/input/saved-model/Emotion_Model.h5\")\n","print(\"Loaded model from disk\")\n","\n","# the optimiser\n","opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n","loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# loading json and model architecture \n","json_file = open('/kaggle/input/saved-model/model_json.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","\n","# load weights into new model\n","loaded_model.load_weights(\"/kaggle/input/saved-model/Emotion_Model.h5\")\n","print(\"Loaded model from disk\")\n","\n","# the optimiser\n","opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n","loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
